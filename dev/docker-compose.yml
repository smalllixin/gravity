services:
  minio:
    image: minio/minio:latest
    container_name: litellm_minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # MinIO Console
    volumes:
      - minio_data:/data

  minio-setup:
    image: minio/mc:latest
    container_name: litellm_minio_setup
    depends_on:
      - minio
    entrypoint: ["/bin/sh", "-c"]
    command: >-
      "sleep 5 &&
      until mc alias set local http://minio:9000 minioadmin minioadmin; do
        echo 'Waiting for MinIO...';
        sleep 2;
      done &&
      mc mb -p local/traces || true &&
      echo 'Bucket created successfully' &&
      mc ls local/"
  litellm:
    build:
      context: .
      args:
        target: runtime
    image: ghcr.io/berriai/litellm:main-latest
    #########################################
    ## Uncomment these lines to start proxy with a config.yaml file ##
    volumes:
     - ./config.yaml:/app/config.yaml
     - ./custom_callbacks.py:/app/custom_callbacks.py
     - ./openinference_otel.py:/app/openinference_otel.py
    command:
     - "--config=/app/config.yaml"
    ##############################################
    ports:
      - "4000:4000" # Map the container port to the host, change the host port if necessary
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@db:5432/litellm"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
    env_file:
      - .env # Load local .env file
    depends_on:
      - db  # Indicates that this service depends on the 'db' service, ensuring 'db' starts first
    healthcheck:  # Defines the health check configuration for the container
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1" ]  # Command to execute for health check
      interval: 30s  # Perform health check every 30 seconds
      timeout: 10s   # Health check command times out after 10 seconds
      retries: 3     # Retry up to 3 times if health check fails
      start_period: 40s  # Wait 40 seconds after container start before beginning health checks

  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data # Persists Postgres data across container restarts
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10

  otel-collector:
    image: otel/opentelemetry-collector-contrib
    container_name: litellm_otel_collector
    restart: always
    volumes:
      - ./collector-config.yaml:/etc/otelcol-contrib/config.yaml
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "8889:8889"  # Prometheus metrics exporter
    environment:
      # AWS SDK envs for S3 exporter to talk to MinIO
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_ENDPOINT_URL: http://minio:9000
    depends_on:
      - minio-setup
      - jaeger
      - prometheus
      - loki

  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: litellm_jaeger
    restart: always
    ports:
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # Jaeger native gRPC receiver
    # Note: Port 4317 is NOT exposed to host to avoid conflict with collector
    # The collector connects to jaeger:4317 internally via Docker network
    environment:
      - COLLECTOR_OTLP_ENABLED=true

  prometheus:
    image: prom/prometheus
    container_name: litellm_prometheus
    restart: always
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"

  loki:
    image: grafana/loki:latest
    container_name: litellm_loki
    restart: always
    ports:
      - "3100:3100"  # Loki API
    command: -config.file=/etc/loki/config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
      - loki_data:/loki

  grafana:
    image: grafana/grafana:latest
    container_name: litellm_grafana
    restart: always
    ports:
      - "3001:3000"  # Grafana UI (using 3001 to avoid conflict)
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
      - jaeger
      - loki

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
  postgres_data:
    name: litellm_postgres_data # Named volume for Postgres data persistence
  minio_data:
    driver: local
